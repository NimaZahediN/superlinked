---
title: "Using the API"
icon: brackets-curly
---

Once you have your application up and running, you can start loading data and querying the API. Follow these steps to interact with your Superlinked application:

<Steps>
<Step title="Ingest an entry">
  Test data ingestion by making a POST request to the `/api/v1/ingest/your_schema` endpoint:

```bash
curl -X POST \
    'http://localhost:8080/api/v1/ingest/your_schema' \
    --header 'Accept: */*' \
    --header 'Content-Type: application/json' \
    --data-raw '{
        "id": "your_id",
        ...
    }'
```

  <Note>
  The current example will not work as-is. Please change the request body according to your schema requirements.
  </Note>
</Step>

<Step title="Query your system">
  After ingesting data, query the API by making a POST request to the `/api/v1/search/query` endpoint:

```bash
curl -X POST \
    'http://localhost:8080/api/v1/search/query' \
    --header 'Accept: */*' \
    --header 'Content-Type: application/json' \
    --header 'x-include-metadata: true' \
    --data-raw '{
        "query_text": "your_search_text"
    }'
```

  <Tip>
  The `x-include-metadata: true` header will include additional debug information about your query, such as search vector, weights and evaluated NLQ outputs.
  </Tip>
</Step>

<Step title="See available data loaders">
  To see what data loaders are available, send a request to the data loader endpoint:

```bash
curl 'http://localhost:8080/data-loader/'
```

Successful response (200 OK):

```json
{
    "result": [
        "<NAME_OF_YOUR_DATA_LOADER>": "DataLoaderConfig(path='https://path-to-your-file.csv', format=<DataFormat.CSV: 2>, name=None, pandas_read_kwargs='{sep: ;}')"
    ]
}
```

  <Info>
  The keys are the available data loader names that you can use for the data loader endpoints. To see how these names are constructed and can be altered, read the [docs here](configuring-your-app.md#incorporate-data-source).
  </Info>
</Step>

<Step title="Trigger the data load">
  Initiate the data load by invoking its endpoint. This will spawn an asynchronous task `DataLoaderSource` by its name as defined in your `api.py`:

```bash
curl -X POST 'http://localhost:8080/data-loader/<NAME_OF_YOUR_DATA_LOADER>/run'
```

Successful response (202 Accepted):

```json
{
  "result": "Background task successfully started with name: <NAME_OF_YOUR_DATA_LOADER>"
}
```

  <Warning>
  If the name you provided is not found in the system, a 404 NOT FOUND will be returned. Check the logs to verify the result of the task.
  </Warning>
</Step>
</Steps>
